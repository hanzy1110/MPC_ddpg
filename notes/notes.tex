% Created 2023-09-14 jue 14:16
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Enrique Loeser}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Enrique Loeser},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.7)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{Notes for the project:}
\label{sec:orgede8d68}

\subsection{MPC Control:}
\label{sec:orgbd498e3}
I have a state space model for the mpc control used to control the opening of a valve
Out variables should control last\textsubscript{output}\textsubscript{velocity}, last\textsubscript{output}\textsubscript{displacemnt}
How does MPC control work?

\begin{equation}
    \dot{x}(t) &= Ax(t) + Bu(t)
\end{equation}

\begin{equation}
    y(t) &= Cx(t) + Du(t)
\end{equation}

\subsubsection{MPC Control}
\label{sec:org4f90111}
 MPC is based on iterative, finite-horizon optimization of a plant model. At time t t the current plant state is sampled and a cost minimizing control strategy is computed (via a numerical minimization algorithm) for a relatively short time horizon in the future: [ t , t + T ] [t,t+T]
Model predictive control is a multivariable control algorithm that uses:

\begin{itemize}
\item    an internal dynamic model of the process
\item    a cost function J over the receding horizon
\item    an optimization algorithm minimizing the cost function J using the control input u
\end{itemize}

\begin{enumerate}
\item Equations:
\label{sec:org3a49717}

\(n_x = 6, N = 1\)

\(shape(x) = (6, 1)\)
\(shape(u) = (3, 1)\)

\begin{itemize}
    \item Variables $x, u$
    \item constraints:
    \begin{itemize}
        \item $x_{k+1} = A_d x_k + B_d u_k$
        \item $u_k >= -10$, $u_k<0$
    \end{itemize}
\end{itemize}

Controlup outputs the predicted control signal \(u[:,0]\)

\bold{Adapt the control methods to provide the predicted state of the system to the DDPG framework}
\end{enumerate}

\subsection{DDPG:}
\label{sec:org3aa477f}
Reinforcement learning technique that strives to combine perception capabilities of Deep learning with Decision capabilities of conventional reinforcement learning.

\begin{itemize}
    \item The perception step that gives information about the enviroment.
    \item The decision step that self actualices to get an appropiate response.
\end{itemize}

\begin{enumerate}
\item How to define agents
\label{sec:org089b7c5}

\begin{itemize}
    \item We need to deal with distinct and continous action spaces
    \item State spaces:

    \begin{itemize}
        \item controllerup: $[-10, 0]$
        \item controllerdown: $[0, 1]$
        \item controllerup: $[0, 250]$
    \end{itemize}
\end{itemize}
\end{enumerate}

\subsection{Data flow:}
\label{sec:orgd712193}

\begin{itemize}
    \item MPC: Takes in current state $x_k$ and a target reference
    \item MPC: outputs predicted state $x_{k+1}$, predicted control $u_{k+1}$
    \item DDPG Takes in predicted state and outputs a control action $u_{k+1}$
    \item Why should I Update target reference?
\end{itemize}
\end{document}